---
title: "Step 3a data cleaning"
subtitle: "Dereplicate sequence reads across samples; filter sequences based on length and occurrence; clean sequences of PCR/sequencing errors."
author: "B.Littleford-Colquhoun & T.Divoll"
date: "2023-09-21"
output: 
  html_notebook:
  keep_md: TRUE
  df_print: paged
  toc: true
params:
    #don't change these
    user: !r Sys.getenv("LOGNAME")
    data_path: "/oscar/data/tkartzin/projects"
    
    #update these for your analysis
    step_3a_analysis_date: "20241123" #YYYYMMDD
    project_code: "test"
    seq_run_name: ["URI_20221013", "URI_20230519"] #these are the folder names in the 'data/tkartzin/projects/<project_code>' dir
    step_1_analysis_date: ["20240502_tdivoll", "20240430_tdivoll"] #make sure these dates are in the same order as the appropriate `seq_run_name` folders
    metadata_file: "test_phyloseq_metadata.csv"
    
    ##obitools params
    min_read_length: 8        #-l in obigrep
    max_read_length: 300      #-L in obigrep
    copy_number_threshold: 2  #-p in obigrep
    clean_threshold: 0.05     #-r in obiclean
---

```{r include=FALSE}
## Only needed for knitr to render this notebook
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, here, tidyr, dplyr, janitor, stringr, readr, glue)
here::i_am("./Step3a_data_cleaning.Rmd")
```

```{r setup, include=FALSE}
# set global chunk parameters here
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
Sys.setenv(user=params$user)
Sys.setenv(date=params$step_3a_analysis_date)
Sys.setenv(min_read_length=params$min_read_length)
Sys.setenv(max_read_length=params$max_read_length)
Sys.setenv(copy_threshold=params$copy_number_threshold)
Sys.setenv(clean_threshold=params$clean_threshold)
```

The date for the output path will correspond to this analysis date.
```{r}
output_path = file.path(params$data_path, params$project_code, "merged_runs", sprintf("%s_%s", params$step_3a_analysis_date, params$user))
dir.create(output_path)

Sys.setenv(output_path=output_path)
```

#### Make a list of files we want to process
In our metadata file, the SampleID and Folder columns are useful for filtering down to just the files we want to process rather than pulling everything from each folder.

Check for duplicate sample names. Expect a message like this: "No duplicate combinations found of: SampleID". If you get output in the new `duplicates` dataframe, perhaps you have run the sample before? Recheck your metadata file and remove the duplicate samples.
```{r}
metadata <- read_csv(params$metadata_file)
duplicates <- get_dupes(metadata, SampleID)
```

```{r}
file_list <- list.files(path=file.path(params$data_path, params$project_code, params$seq_run_name, params$step_1_analysis_date), recursive=T, pattern=("*.fastq"), full.names=T)

matching_files <- file_list[sapply(file_list, function(file) {
  any(grepl(paste(metadata$SampleID, collapse = "|"), basename(file)))
  })]
```

#### Collect all the final results from each sequencing run
```{r}
merged_data <- grep("id_data", matching_files, value = TRUE, fixed = TRUE)
```

Copy the collected files over to a dated folder in the project's 'merged_runs' folder.
```{r}
file.copy(from = merged_data, to = output_path)
```

Now that all the controls and potentially bad samples were removed in Step 1c, we rerun the merging steps and clean the final merged data set.

#### Combine remaining sequences into one FASTA and dereplicate
```{bash}
source ~/.bashrc
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obiuniq -m sample *_id.fastq --without-progress-bar > all.uniq.fasta
obicount all.uniq.fasta --without-progress-bar
```

#### Filter results to just `count` and `merged_sample` attributes in FASTA headers
```{bash}
source ~/.bashrc
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obiannotate -k count -k merged_sample all.uniq.fasta > $$ ; mv $$ all.uniq.fasta
```

#### Inspect the `count` attribute
```{bash}
source ~/.bashrc
source activate /users/${user}/.conda/envs/obitools-env
cd $output_path

# get the counting statistics on the ‘count’ attribute
{
obistat -c count all.uniq.fasta --without-progress-bar | sort -nk1 | { head -n 20; tail -n 20; } | column -t 
} | tee -a seq_count_table_head_no_controls.txt
```

#### Create a tabulated file for counts per sequence read
```{bash}
source ~/.bashrc
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obitab -o all.uniq.fasta > all.uniq.tab
```

#### Filter out sequences by abundance and length
Keep only the sequences having a count greater or equal to `min_read_length` and a length shorter than `max_read_length` bp (this should be changed depending on marker and filtering criteria)

```{bash}
source ~/.bashrc
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obigrep -l $min_read_length -L $max_read_length -p "count>=$copy_threshold" all.uniq.fasta > ${date}_all.uniq.fasta
```

#### Clean the sequences for PCR/sequencing errors (sequence variants)
We keep the head sequences (-H option) that are sequences with no variants with a count greater than 5% of their own count (-r 0.05 option). 
**Note:**This step takes a while.
```{bash}
source ~/.bashrc
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obiclean -s merged_sample -r $clean_threshold -H ${date}_all.uniq.fasta > ${date}_alluniq.clean.fasta
```
#### Create a tabulated file to check the number of sequence reads in each sample

```{bash}
source ~/.bashrc
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obitab -o ${date}_alluniq.clean.fasta > ${date}_alluniq.clean.tab
obicount ${date}_alluniq.clean.fasta --without-progress-bar
```
#### Calculate mean and range of sample depths
```{r}
 tabseqs <- read_tsv(list.files(output_path, pattern = "_alluniq.clean.tab", full.names=T))
sample_depth <- colSums(tabseqs[,grep("^sample:", colnames(tabseqs))])
```

```{r}
print(paste("The minimum sample depth is: ", min(sample_depth)))
print(paste("The maximum sample depth is: ", max(sample_depth)))
print(paste("The mean sample depth is: ", mean(sample_depth)))
```


#### Calculate mean and range sequence lengths for each sample
After this has run, look at the minimum, maximum, and mean sequence lengths for reporting.
```{r}
seq_lengths <- tabseqs[, grep("^sequence", names(tabseqs))]
seq_lengths$length <- str_length(seq_lengths$sequence)
print(paste("The mean sequence length is: ", signif(mean(seq_lengths$length))))
print(paste("The minimum sequence length is:", min(seq_lengths$length)))
print(paste("The maximum sequence length is: ", max(seq_lengths$length)))

```

#### Take notes
These notebooks will remain in your dated analysis folder, use this space for any final notes on the cleaned data set:



<br>

<br> 


#### Copy over your notebook to the shared lab directory on Oscar.

**Note: Save and knit this notebook first so any results are recorded before copying. Then run this final chunk.**
```{r}
file.copy("./Step3a_data_cleaning.nb.html", file.path(glue("{output_path}/Step3a_data_cleaning.nb.html")), overwrite = TRUE)
```