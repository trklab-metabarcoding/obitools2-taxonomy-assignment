---
title: "Step 3a data cleaning"
author: "B.Littleford-Colquhoun & T.Divoll"
date: "2023-09-21"
output: 
  html_document:
  keep_md: TRUE
  df_print: paged
  toc: true
params:
    #don't change these
    todaydate: !r (format(Sys.Date(), "%Y%m%d"))
    user: !r Sys.getenv("LOGNAME")
    data_path: "/oscar/data/tkartzin/projects"
    
    #update these for your analysis
    project_code: "test"
    seq_run_name: ["URI_20221013", "URI_20230519"] #these are the folder names in the 'data/tkartzin/projects/<project_code>' dir
    step_1_analysis_date: ["20240502_tdivoll", "20240430_tdivoll"] #make sure these dates are in the same order as the appropriate `seq_run_name` folders
    
    ##obitools params
    min_read_length: 8     #-l in obigrep
    max_read_length: 300   #-L in obigrep
    clean_threshold: 0.05  #-r in obiclean
---

```{r include=FALSE}
## Only needed for knitr to render this notebook
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, here, tidyr, dplyr, janitor, stringr)
here::i_am("./Step3a_data_cleaning.Rmd")
```

```{r setup, include=FALSE}
# set global chunk parameters here
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(user=params$user)
Sys.setenv(todaydate=params$todaydate)
Sys.setenv(min_read_length=params$min_read_length)
Sys.setenv(max_read_length=params$max_read_length)
Sys.setenv(clean_threshold=params$clean_threshold)
```

The date for the output path will correspond to this analysis date.
```{r}
output_path = file.path(params$data_path, params$project_code, "merged_runs", sprintf("%s_%s", params$todaydate, params$user))
dir.create(output_path)

Sys.setenv(output_path=output_path)
```

#### Collect all the final results from each sequencing run
```{r}
file_list <- list.files(path=file.path(params$data_path, params$project_code, params$seq_run_name, params$step_1_analysis_date),
           recursive=T,
           pattern=("*.fastq"),
           full.names=T)
merged_data <- grep("id_data", file_list, value = TRUE, fixed = TRUE)
```

Copy the collected files over to a dated folder in the project's 'merged_runs' folder.
```{r}
file.copy(from = merged_data, to = output_path)
```

Check for duplicate sample names. Expect a message like this: "No duplicate combinations found of: sample_names". If you get output here, perhaps you have run the sample before? 
```{r }
files <- list.files(path=output_path, pattern="\\.fastq$")
samples_in_files <- as.data.frame(str_extract(files, "[^_]+"))
samples_in_metadata <-  #exclude and remove non-matching

colnames(samples) <- "sample_names"
duplicates <- get_dupes(samples, sample_names)
```

Now that all the controls and potentially bad samples were removed in Step 1c, we rerun the merging steps and clean the final merged data set.

#### Combine remaining sequences into one FASTA and dereplicate
```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obiuniq -m sample *_id.fastq --without-progress-bar > all.uniq.fasta
obicount all.uniq.fasta --without-progress-bar
```

#### Filter results to just `count` and `merged_sample` attributes in FASTA headers
```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obiannotate -k count -k merged_sample all.uniq.fasta > $$ ; mv $$ all.uniq.fasta
```

#### Inspect the `count` attribute
```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

##### test with head & tail vs. two separate lines

# get the counting statistics on the ‘count’ attribute
{
obistat -c count all.uniq.fasta --without-progress-bar | sort -nk1 | head -20 | column -t 
} | tee -a seq_count_table_head_no_controls.txt

{
obistat -c count all.uniq.fasta --without-progress-bar | sort -nk1 | tail -20 | column -t 
} | tee -a seq_count_table_tail_no_controls.txt
```

#### Create a tabulated file for counts per sequence read
```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obitab -o all.uniq.fasta > all.uniq.tab
```

#### Filter out sequences by abundance and length
Keep only the sequences having a count greater or equal to `min_read_length` and a length shorter than `max_read_length` bp (this should be changed depending on marker and filtering criteria)

```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obigrep -l $min_read_length -L $max_read_length -p 'count>=2' all.uniq.fasta > ${todaydate}_all.uniq.fasta
```

#### Clean the sequences for PCR/sequencing errors (sequence variants)
We keep the head sequences (-H option) that are sequences with no variants with a count greater than 5% of their own count (-r 0.05 option). 
**Note:**This step takes a while.
```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obiclean -s merged_sample -r $clean_threshold -H ${todaydate}_all.uniq.fasta > ${todaydate}_alluniq.clean.fasta
```
#### Create a tabulated file to check the number of sequence reads in each sample

```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obitab -o ${todaydate}_alluniq.clean.fasta > ${todaydate}_alluniq.clean.tab
obicount ${todaydate}_alluniq.clean.fasta --without-progress-bar
```

#### Take notes
These notebooks will remain in your dated analysis folder, use this space for any final notes on the cleaned data set: