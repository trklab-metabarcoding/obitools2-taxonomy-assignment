---
title: "Step 4b - Phyloseq global and local"
subtitle: "Create phyloseq object using global and local output"
authors: "TR Kartzinel, BL Littleford-Colquhoun, HK Hoff, TJ Divoll"
date: "2025-03-20"
output:
  html_document:
    df_print: paged
  keep_md: yes
  df_print: paged
  toc: yes
  html_notebook: null
params:
  # don't change these
  todaydate: !r (format(Sys.Date(), '%Y%m%d'))
  user: !r Sys.getenv('LOGNAME')
  data_path: /oscar/data/tkartzin/projects
  global_ref_lib_path: /oscar/data/tkartzin/global_ref_lib
  local_ref_lib_path: /oscar/data/tkartzin/local_ref_lib
  
  # update these for your analysis
  project_code: "test"
  taxa_division: "PLN"
  locus: "trnL"
  region_of_interest: "P6"
  latest_db_date: "20241125" #find in `global_ref_lib`
  step_2a_analysis_date: "20241227" #look in the `latest_db_date` folder and get the date associated with the `_completeDB.csv` file
  step_2b_analysis_date: "20241108" #look in the `local_ref_lib` folder under the desired project code, locus, and region of interest to get the date of the folder associated with the `_completeDB.csv` file you would like to use
  step_3a_analysis_date: "20240502" #look in tkartzin > projects > project_code > merged_runs > date on folder YYYYMMDD_username
  step_3b_analysis_date: '20240502' #if running this step on the same day as Step 3, this will be `todaydate`, otherwise enter the date when you ran Step 3b
  step_3c_analysis_date: '20240502' #if running this step on the same day as Step 3, this will be `todaydate`, otherwise enter the date when you ran Step 3b
  metadata_file: "test_phyloseq_metadata.csv"
  percent_match: 1
  min_read_count: 1000 # threshold to catch cleaned samples with low read counts
  RRA_threshold: 0.05
  sampleID_column: 'SampleID'
  
  # major and minor groupings are variables of interest to determine sample size statistics and grouping for post-processing
  major_grouping: 'Species'
  minor_grouping: 'Season' # could also be Location or Collection Type, etc.
---

This notebook will use your data with taxonomy assigned from the latest global reference database as well as the latest project-specific local database to create a Phyloseq object for downstream analyses.
```{r setup, include=FALSE}
## Only needed for knitr to render this notebook
knitr::opts_chunk$set(echo = TRUE)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, here, tidyr, dplyr, vegan, vegetarian, scales, datawizard, ggplot2, ggrepel, glue, textshape, stringr, forcats)

if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("phyloseq")

here::i_am("./Step4a_create_phyloseq_global.Rmd")
```

<br>

## Read in data
This code block will use the tabulated file created in Step 3 and a list of samples; it loads the cleaned global and local databases.
```{r}
input_path <- file.path(params$data_path, params$project_code, "merged_runs", sprintf("%s_%s", params$step_3a_analysis_date, params$user))

data_global <- read.csv(file.path(input_path, sprintf("%s_global_alluniq.clean.tag.ann.tab", params$step_3b_analysis_date)), sep="\t", header=TRUE)

data_local <- read.csv(file.path(input_path, sprintf("%s_local_alluniq.clean.tag.ann.tab", params$step_3c_analysis_date)), sep="\t", header=TRUE)

data_samples <- read.csv(params$metadata_file)
```

<br>

### Collect some insights on the shape of data
Are there the same number of sequences in your local and global files?
```{r}
if (length(which(data_local$id != data_global$id)) == 0){
  print("Your files have the same number of sequences.")
} else {
  print("There is a mismatch in file lengths. Please check inputs and rerun Step 3.")
}
```

<br>

### Rename the default column X to `r params$sampleID_column` 
```{r}
colnames(data_samples)[colnames(data_samples) == "X"] = params$sampleID_column
```

<br>

This block of code will read in the 'completeDB.csv' global library output of step 2a, trim this file to sequence, merged_family, merged_genus, and merged_species columns, and join this file to 'data_global' by sequence.
```{r}
# Read in .csv file from step 2a
global_detailed_headers <- read.csv(file.path(params$global_ref_lib_path, params$taxa_division, params$locus, params$region_of_interest, params$latest_db_date, paste0(params$step_2a_analysis_date, "_", params$region_of_interest, "_completeDB.csv")))
head(global_detailed_headers)

# Trim columns 
global_detailed_headers <- data.frame(sequence = global_detailed_headers$sequence, merged_family_name = global_detailed_headers$merged_family_name, merged_genus_name = global_detailed_headers$merged_genus_name, merged_species_name = global_detailed_headers$merged_species_name)
ncol(global_detailed_headers)

# Join global_detailed_headers to data_global
data_global <- left_join(data_global, global_detailed_headers, by = "sequence")
```

This block of code will read in the 'completeDB.csv' local library output of step 2b, trim this file to sequence, merged_family, merged_genus, and merged_species columns, and join this file to 'data_local' by sequence.
```{R}
# Read in .csv file from step 2b
local_detailed_headers <- read.csv(file.path(params$local_ref_lib_path, params$project_code, params$locus, params$region_of_interest, params$step_2b_analysis_date, paste0(params$project_code, params$region_of_interest, "_", params$step_2b_analysis_date, "_completeDB.csv")))
head(local_detailed_headers)

# Trim columns
local_detailed_headers <- data.frame(sequence = local_detailed_headers$sequence, merged_family_name = local_detailed_headers$merged_family_name, merged_genus_name = local_detailed_headers$merged_genus_name, merged_species_name = local_detailed_headers$merged_species_name)
ncol(local_detailed_headers)

# Join local_detailed_headers to data_local
data_local <- left_join(data_local, local_detailed_headers, by = "sequence")
```

<br>

### Filter the data to samples with at least `r params$min_read_count`
```{r}
filter_low_count_samples <- function(df) {
  # calc read count sums for sample columns
  sample_sums <- df %>% 
    select(starts_with("sample.")) %>% 
    summarise(across(everything(), \(x) sum(x, na.rm = TRUE)))
  
  # get names of columns to keep (those with sums >= `min_read_count`)
  columns_to_keep <- sample_sums %>%
    pivot_longer(everything(), names_to = "column", values_to = "sum") %>%
    filter(sum >= params$min_read_count) %>%
    pull(column)
  
  # get all non-sample columns to keep
  non_sample_cols <- names(df)[!grepl("^sample\\.", names(df))]
  
  # return the filtered df with only samples passing `min_read_count`
  df %>% select(all_of(non_sample_cols), any_of(columns_to_keep))
}
```

### Filter the data to only keep 100% matches and samples >`min_read_count` in the local set
```{r}
size <- dim(data_local) # number of rows and columns
print(paste0("Number of rows in local data: ", size[1]))
print(paste0("Number of columns in local data: ", size[2]))

best_id_col_local <- find_columns(data_local, starts_with("best_identity"))
best_match_col_local <- find_columns(data_local, starts_with("best_match"))

data_local <- data_local %>% 
  select(-contains("obiclean_status"))

# filter for percent match
local_keep <- data_local[which(data_local[[best_id_col_local]]==params$percent_match),]
# filter for min read count
local_keep <- filter_low_count_samples(local_keep)

local_keep_rows <- nrow(local_keep)
print(paste0("Number of local rows kept: ", local_keep_rows))
```

<br>

### Filter the data to only keep 100% matches and samples >`min_read_count` in the global set
Global only keeps perfect matches if they are not already present in the local set of matches.
```{r}
size <- dim(data_global) # number of rows and columns
print(paste0("Number of rows in global data: ", size[1]))
print(paste0("Number of columns in global data: ", size[2]))

best_id_col <- find_columns(data_global, starts_with("best_identity"))
best_match_col <- find_columns(data_global, starts_with("best_match"))

data_global <- data_global %>% 
  select(-contains("obiclean_status"))
  
# filter for any new matches at the percent match threshold if they are not found in the local set already
global_keep <- data_global[intersect(which(data_global[[best_id_col]] == params$percent_match), which(data_local[[best_id_col_local]]!= params$percent_match)),]

# filter for min read count per sample
global_keep <- filter_low_count_samples(global_keep)

global_keep$scientific_name <- make.unique(as.character(global_keep$scientific_name))

print(paste0("Number of global rows kept: ", nrow(global_keep)))
```

<br>

### Combine the local and global datasets
Combine local and additional global libraries into a database called "keep" for exploratory analyses.
```{r}
# Add missing columns to global before overwriting column names
global_keepKEEP <- global_keep %>%
    mutate(merged_family_name = "", 
           merged_genus_name = "",
           merged_species_name = "",
           .after = "sequence")

# Overwrite with local names
names(global_keepKEEP) <- names(local_keep)
keep <- rbind(local_keep, global_keepKEEP)
```

<br>

### Explore the distribution of sequences
Find out distribution of the sequences you toss both locally and globally (i.e., do some match multiple species perfectly, or anything new in GenBank?).
```{r}
local_global_toss <- data_local[which(data_local$id %in% keep$id==F),] #tells you how many taxa didn't have 100% matches 

print(paste0("The number of sequences not matching at 100%: ", dim(local_global_toss)[1]))
```

<br>

### Look at the seqs with highest read counts for matches >98%
```{r}
local_global_toss <- local_global_toss[order(local_global_toss$count,decreasing=T),]
head(local_global_toss)
# may be a couple of taxa we should look at that have 1-2 matches at ~98% match with high read count 
```

<br>

### Look at a histogram of %match < 1 for the tossed samples
```{r warning=FALSE}
ggplot(local_global_toss, aes_string(x = best_id_col_local)) +
  geom_histogram(binwidth=0.05, fill = "steelblue", color = "black") +
  labs(title = "Histogram of Best ID Column Local",
       x = best_id_col_local,
       y = "Frequency") +
  scale_x_continuous(
  breaks = seq(0, 1, by = 0.1),
  labels = scales::percent_format(scale = 100)
) + 
  theme_minimal()
```

<br>

### Count up 100% matches in each family with the local library
```{r}
local_keep %>% count(family_name, sort = TRUE)
```

<br>

### Count up 100% matches in each genera with the local library
```{r}
local_keep %>% count(genus_name, sort = TRUE)
```

<br>

### Count up 100% matches in each family with the global library
```{r}
global_keep_families <- global_keep %>% count(family_name, sort = TRUE)
global_keep_families
write.csv(global_keep_families, file.path(input_path, "global_keep_familes.csv"))
```

<br>

### Count up 100% matches in each genera with the global library
```{r}
global_keep_genera <- global_keep %>% count(genus_name, sort = TRUE)
global_keep_genera
write.csv(global_keep_genera, file.path(input_path, "global_keep_genera.csv"))
```

<br>

## Inspect the 90-99% matches
```{r}
local_nineties <- filter(data_local, between(data_local[[best_id_col_local]], 0.9, 1)) %>%
  arrange(desc(best_id_col_local))
local_nineties %>% count(family_name, sort = TRUE) # families with lots of reads matching at 90-99.99% 
```

```{r}
global_nineties <- filter(data_global, between(data_global[[best_id_col]], 0.9, 1)) %>%
  arrange(desc(best_id_col))
global_nineties %>% count(family_name, sort = TRUE) # families with lots of reads matching at 90-99.99% 
```

<br>

Filter for records in the global that are a 100% match not already matched at 100% in the local db.
```{r}
global_nineties_keep <- global_nineties[intersect(which(global_nineties[[best_id_col]] == params$percent_match), which(data_local[[best_id_col_local]] != params$percent_match)),]
```

<br>

## Generate summary statistics for publication
#### Median, mean, and max for local and global counts
```{r}
# Mean read depth of taxa identified using the local reference library
print(paste0("The mean depth across all taxa for the local keep subset: ", round(mean(local_keep$count))))

# Mean read depth of taxa identified using the global reference library
print(paste0("The mean depth across all taxa for the global keep subset: ", round(mean(global_keep$count))))

# Median read depth of taxa identified using the local reference library
print(paste0("The median depth across all taxa for the local keep subset: ", round(median(local_keep$count))))

# Median read depth of taxa identified using the global reference library
print(paste0("The median depth across all taxa for the global keep subset: ", round(median(global_keep$count))))

# Maximum read depth of taxa identified using the local reference library
print(paste0("The maximum depth for any one taxa in the local keep subset: ", max(local_keep$count))) 

# Maximum read depth of taxa identified using the global reference library
print(paste0("The maximum depth for any one taxa in the global keep subset: ", max(global_keep$count)))
```

<br>

### Comparison of median read depths across local and global reference libraries
```{r}
## This assumes your local reference has better median read depth than the global. Some projects may still have a small local database relative to global, so you could switch global and local in the division below, and make sure to report that the global is x-fold greater than local.
print(paste0("Median read depths for the local reference library were >",round(median(local_keep$count)/median(global_keep$count)),"-fold greater than the global reference library"))

# Calculate total number of reads in final dataset that were taxonomically assigned using each reference library
print(paste0("Total number of reads in the global keep subset: ", sum(global_keep$count)))
print(paste0("Total number of reads in the local keep subset: ", sum(local_keep$count)))
print(paste0("Total number of reads in the final dataset: ", sum(global_keep$count) + sum(local_keep$count)))
```

<br>

#### Sample read depths
This will make tables with the depths for each sample in each data subset.
```{r}
depth <- as.data.frame(t(colSums(data_local[,grep("sample\\.", colnames(data_local))])))
depth_local_only <- as.data.frame(t(colSums(local_keep[,grep("sample\\.", colnames(local_keep))])))
depth_global_only <- as.data.frame(t(colSums(global_keep[,grep("sample\\.", colnames(global_keep))])))
depth_local_and_global <- as.data.frame(t(colSums(keep[,grep("sample\\.", colnames(keep))])))

# Combine into one result df; view the new 'sample_depth' dataframe for comparisons
sample_depths <- bind_rows(
  depth = depth,
  depth_local_only = depth_local_only,
  depth_global_only = depth_global_only,
  depth_local_and_global = depth_local_and_global,
  .id = "source"
)
```

<br>

#### Proportions discarded with each subset
```{r}
sample_depths <- sample_depths %>%
  pivot_longer(cols = -source, names_to = "sample", values_to = "value") %>%
  pivot_wider(names_from = source, values_from = value) %>%
  group_by(sample) %>%
  mutate(prop_kept_local = depth_local_only / depth,
         prop_kept_global = depth_global_only / depth,
         prop_kept_both = depth_local_and_global / depth,
         reads_discarded = 1 - prop_kept_both,
         !!sym(params$sampleID_column) := substring(sample, 8, 13)) %>%
  ungroup() %>%
  left_join(data_samples %>%
              select(!!sym(params$sampleID_column), !!sym(params$major_grouping)), by = setNames(params$sampleID_column, params$sampleID_column))

write.csv(sample_depths, "./sample_depths.csv")
sample_depths 
```
<br>

#### Plot proportional changes at sample level if you add or don't add the global perfect matches to the local perfect matches
```{r}

ggplot(sample_depths, aes(x = prop_kept_local, y = prop_kept_both)) +
  geom_point(size=3) +
  geom_text_repel(aes(label = !!sym(params$sampleID_column)),
                  size=3)+
  labs(x = "Proportion Kept Locally", 
       y = "Proportion Kept Both Locally and Globally") +
  scale_y_continuous(
  labels = percent, 
  limits = c(0, 1), 
) +
  scale_x_continuous(
  labels = percent, 
  limits = c(0, 1), 
) +
  theme_minimal()
```

<br>

#### Plot proportions from each subset by major gouping
```{r warning=FALSE}
sample_depths %>% 
  gather(type, count, prop_kept_local:prop_kept_global:reads_discarded) %>% 
  filter(type != "prop_kept_both") %>%
  ggplot(., aes(x=sample, y=count, fill=forcats::fct_rev(type))) +
  scale_fill_manual(values = c("grey50", "#FFC20A", "#0C7BDC")) +
  geom_bar(stat="identity") +
  theme_classic()+
  theme(legend.title=element_blank(),
        axis.text.x=element_blank()) +
  scale_y_continuous(
  labels = percent, 
  limits = c(0, 1), 
  expand = expansion(0, 0)
) +
  facet_grid(reformulate(params$major_grouping), 
           scales = "free", space = "free") + 
  theme(strip.text = element_text(
    size = 5))
```

<br>

#### Print some important metrics overall
```{r}
#should be 90% to 80%
print(paste0("Overall proportion of reads retained with local and global combined: ", sprintf(sum(sample_depths$depth_local_and_global)/sum(sample_depths$depth),fmt='%#.3f')))

#should be 67% to 61%
print(paste0("Proportion of reads retained with just local: ", sprintf(sum(sample_depths$depth_local_only)/sum(sample_depths$depth),fmt='%#.3f')))

#should be 23% to 27%
print(paste0("Additional proportion of reads retained with just global: ", sprintf(sum(sample_depths$depth_global_only)/sum(sample_depths$depth),fmt='%#.3f')))
```

<br>

#### 100% matches in local that were new to global
```{r}
global_match <- data_global[which(data_global[[best_id_col]] == 1),]

global_missing <- subset(local_keep, !(id %in% global_match$id)) %>%
  arrange(desc(count))

global_missing_trim <- select(global_missing, starts_with("sample."))

global_missing_RRA <- as.matrix(global_missing_trim) %o% as.matrix(1/sample_depths$depth_local_and_global)

global_missing_RRA_max <- round(apply(global_missing_RRA, 1, max, na.rm=TRUE), digits=6)
global_missing_RRA_mean <- round(apply(global_missing_RRA, 1, mean, na.rm=TRUE), digits=6)

global_missing_RRA_info <- cbind(global_missing %>% select(sequence, taxid, species_name, count, genus_name, family_name, starts_with(c("species_list", "best_", "match_"))), global_missing_RRA_max, global_missing_RRA_mean)
global_missing_RRA_info
```

<br>

## Sample-wise Relative Read Abundance
Sample-wise distribution of reads retained/discarded based on local and/or global criteria.
```{r}
# Calculate range of RRA change when global matches are added in
additional_RRA_with_global <- round(sample_depths$prop_kept_both - sample_depths$prop_kept_local, digits=3)
RRA_range_global <-  range(additional_RRA_with_global)
print(glue("Range of RRA with global: ", RRA_range_global[1], "-", RRA_range_global[2]))
print(glue("Median RRA with global across samples: ", median(additional_RRA_with_global)))
print(glue("Mean RRA with global across samples: ", mean(additional_RRA_with_global)))

# If >75% of samples gain less than 1% of reads back by including the global, we're in good shape; however, a subset of samples (<5%) could gain back a rather high percentage (>61%), suggesting that we should facilitate
print(glue("Quantiles of RRA with global: "))
quantile(additional_RRA_with_global,c(0.75, 0.95,1))
```

<br>

## Disproportionality
Figure out whether any of the global matches contribute disproportionately. Report global-matching-taxa that contribute more than 5% of a sample's reads; within those samples, what are the mean, median, and max values?
```{r}
# This block makes an empty matrix with as many rows as we need for each sequence identified in the global_keep subset
global_keep_plants <- matrix(nrow=nrow(global_keep), ncol=4)
rownames(global_keep_plants) <- as.character(global_keep$scientific_name)
colnames(global_keep_plants) <- c("length", "mean", "median", "maximum")
```

<br>

This block will loop through each taxa and calculate metrics for each taxa that has at least 1 sample with RRA > RRA_threshold.  
```{r}
for(i in 1:nrow(global_keep_plants)){
  sample_cols = global_keep[i,grep("sample\\.", colnames(global_keep))]
  global_keep_plants[i,1] <- length(which((sample_cols/depth) > params$RRA_threshold))
    
  if(global_keep_plants[i, "length"] >= 1){
    
      global_keep_plants[i, "mean"] <- mean(as.numeric(sample_cols[which((sample_cols/depth) > params$RRA_threshold)])  /  as.numeric(depth[which((sample_cols/depth) > params$RRA_threshold)]))
    
      global_keep_plants[i, "median"] <- median(as.numeric(global_keep[i, grep("sample\\.", colnames(global_keep))][which((sample_cols/depth) > params$RRA_threshold)])  /  as.numeric(depth[which((sample_cols/depth) > params$RRA_threshold)]))
    
      global_keep_plants[i, "maximum"] <- max(as.numeric(global_keep[i, grep("sample\\.", colnames(global_keep))][which((sample_cols/depth) > params$RRA_threshold)])  /  as.numeric(depth[which((sample_cols/depth) > params$RRA_threshold)]))
  }
}

global_keep_plants <- as.data.frame(global_keep_plants) %>% drop_na()
```

<br>

Print out some overall metrics about disproportionality in the dataset.
```{r}
print(glue("Number of disproportionate taxa: ", nrow(global_keep_plants)))

range_of_dis_lengths <- range(global_keep_plants$length)
print(glue("Range of sequence counts: ", range_of_dis_lengths[1], "-", range_of_dis_lengths[2]))

range_of_dis_means <- round(range(global_keep_plants$mean), 3)
print(glue("Range of means: ", range_of_dis_means[1], "-", range_of_dis_means[2]))

range_of_dis_maxs <- round(range(global_keep_plants$maximum), 3)
print(glue("Range of maximums: ", range_of_dis_maxs[1], "-", range_of_dis_maxs[2]))
```

<br>

## Summarize local and non-trace global
Make a new dataframe containing the local matches and the non-trace global matches and calculate summary stats. Non-trace global matches = higher abundance global matches (e.g. >5%).
```{r}
# Add missing columns to global before overwriting column names
global_nontrace <- global_keep[which(global_keep$scientific_name%in%rownames(global_keep_plants)),] %>%
    mutate(merged_family_name = "", 
           merged_genus_name = "",
           merged_species_name = "",
           .after = "sequence")
colnames(global_nontrace) <- names(local_keep)
global_nontrace 
local_nontrace_global <- rbind(local_keep, global_nontrace)
lng_dims <- dim(local_nontrace_global)
print(glue("There are ", lng_dims[1], " sequence records in the local + global_nontrace subset and ", lng_dims[2], " columns."))
```

<br>

Calculate sample-wise retention of RRA.
```{r}
local_nontrace_global_depth <- colSums(local_nontrace_global[,grep("sample\\.", colnames(local_nontrace_global))])

proportion_depth_kept_lng <- local_nontrace_global_depth/as.matrix(depth)
as.data.frame(proportion_depth_kept_lng)
```

<br>

Print metrics about sample-wise retention of RRA.
```{r}
print(glue("Quantiles of RRA with local + global nontrace: "))
quantile(proportion_depth_kept_lng)

# proportion of overall reads kept
print(glue("\n       
Total local + global notrace depth: ", sum(local_nontrace_global_depth)))
print(glue("Total local + global notrace depth retained: ", round(sum(local_nontrace_global_depth)/sum(as.matrix(depth)), 3)))
```

<br>

## Summary statistics for sample groupings
```{r}
# Clean up sample names by removing "sample. and _Sx"
depth_data <- tibble(
  SampleID = str_remove(names(local_nontrace_global_depth), "^sample\\."),
  local_nontrace_global_depth = local_nontrace_global_depth
) %>%
  mutate(SampleID = str_remove(SampleID, "_S\\d+$"))

# Join with metadata
combined_data <- data_samples %>%
  left_join(depth_data, by = "SampleID")

# Calculate stats by group
local_nontrace_global_stats <- combined_data %>%
  group_by(.data[[params$minor_grouping]], .data[[params$major_grouping]]) %>%
  summarize(
    mean = mean(local_nontrace_global_depth, na.rm = TRUE),
    sd = sd(local_nontrace_global_depth, na.rm = TRUE),
    count = n(),
    .groups = "drop"
  )
local_nontrace_global_stats
```

<br>

## Save outputs and generate Phyloseq object
```{r}
# Make the OTU table.
OTU_table <- keep %>%
  select(starts_with("sample.")) %>%
  rename_with(~ str_remove_all(., "sample.|_S\\d+"), starts_with("sample.")) %>%
  as_tibble(rownames = "row_id") %>%
  mutate(row_id = paste0("P", row_id)) %>%
  column_to_rownames("row_id")

# Make the Taxa table.
Taxa_table <- keep %>%
  select(-starts_with("sample.")) %>%
  relocate(id, definition, all_of(best_id_col_local), all_of(best_match_col_local), count, .after = last_col()) %>%
  as_tibble(rownames = "row_id") %>%
  mutate(row_id = paste0("P", row_id)) %>%
  column_to_rownames("row_id")

# Set the index of the samples data to sample names; Phyloseq needs this to match up the different shapes of OTU, TAX, and Samples tables
rownames(data_samples) <-  data_samples[[params$sampleID_column]]
```

## Calulate per-sample number of taxa
```{r}
per_sample_avg <- OTU_table %>%
  pivot_longer(cols = -1, names_to = "sample", values_to = "value") %>%
  group_by(sample) %>%
  summarize(
    num_taxa = sum(value > 0),
  ) %>%
  arrange(sample)
print(per_sample_avg)
```
## Calculate mean, median, min, and max number of taxa
```{r}
# mean number of taxa across samples
print(paste0("The mean number of taxa across samples: ", round(mean(per_sample_avg$num_taxa))))

# median number of taxa across samples
print(paste0("The median number of taxa across samples: ", round(mean(per_sample_avg$num_taxa))))

# min number of taxa across samples
min_taxa <- min(per_sample_avg$num_taxa)
min_sample <- per_sample_avg$sample[which.min(per_sample_avg$num_taxa)]
print(paste0("The minimum number of taxa in any one sample: ", min_taxa, " in ", min_sample))

# max number of taxa across samples
max_taxa <- max(per_sample_avg$num_taxa)
max_sample <- per_sample_avg$sample[which.max(per_sample_avg$num_taxa)]
print(paste0("The maximum number of taxa in any one sample: ", max_taxa, " in ", max_sample))
```

<br>

Build the Phyloseq diet object, rarefy, and export the rarefied tables for easy loading in future.
```{r}
OTU = otu_table(as.matrix(OTU_table), taxa_are_rows = TRUE)
TAX = tax_table(as.matrix(Taxa_table))
samples = sample_data(data_samples)

filename <- glue({params$project_code}, "_diet_local_global_", {params$todaydate}, ".rds")
phyloseq_object <- phyloseq(OTU, TAX, samples)
phyloseq_object

saveRDS(phyloseq_object, filename)

# WE HAVE A NEW PHYLOSEQ OBJECT!!!
```

