---
title: "Step 4b - Phyloseq global and local"
subtitle: "Create phyloseq object using global and local output"
authors: "TR Kartzinel, BL Littleford-Colquhoun, HK Hoff, TJ Divoll"
date: "2025-03-20"
output:
  html_document:
    df_print: paged
  keep_md: yes
  df_print: paged
  toc: yes
  html_notebook: null
params:
  # don't change these
  todaydate: !r (format(Sys.Date(), '%Y%m%d'))
  user: !r Sys.getenv('LOGNAME')
  data_path: /oscar/data/tkartzin/projects
  global_ref_lib_path: /oscar/data/tkartzin/global_ref_lib
  local_ref_lib_path: /oscar/data/tkartzin/local_ref_lib
  
  # update these for your analysis
  project_code: "test"
  taxa_division: "PLN"
  locus: "trnL"
  region_of_interest: "P6"
  latest_db_date: "20241125" #find in `global_ref_lib`
  step_2a_analysis_date: "20241227" #look in the `latest_db_date` folder and get the date associated with the `_completeDB.csv` file
  step_2b_analysis_date: "20241108" #look in the `local_ref_lib` folder under the desired project code, locus, and region of interest to get the date of the folder associated with the `_completeDB.csv` file you would like to use
  step_3a_analysis_date: "20240502" #look in tkartzin > projects > project_code > merged_runs > date on folder YYYYMMDD_username
  step_3b_analysis_date: '20240502' #if running this step on the same day as Step 3, this will be `todaydate`, otherwise enter the date when you ran Step 3b
  step_3c_analysis_date: '20240502' #if running this step on the same day as Step 3, this will be `todaydate`, otherwise enter the date when you ran Step 3b
  metadata_file: "test_phyloseq_metadata.csv"
  percent_match: 1
  min_read_count: 1000 # threshold to catch cleaned samples with low read counts
  RRA_threshold: 0.05
  sampleID_column: 'SampleID'
  
  # major and minor groupings are variables of interest to determine sample size statistics and grouping for post-processing
  major_grouping: 'Species'
  minor_grouping: 'Season' # could also be Location or Collection Type, etc.
---

This notebook will use your data with taxonomy assigned from the latest global reference database as well as the latest project-specific local database to create a Phyloseq object for downstream analyses.
```{r setup, include=FALSE}
## Only needed for knitr to render this notebook
knitr::opts_chunk$set(echo = TRUE)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, here, tidyr, dplyr, vegan, vegetarian, scales, datawizard, ggplot2, ggrepel, glue, textshape, stringr, forcats)

if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("phyloseq", ask = FALSE)

here::i_am("./Step4a_create_phyloseq_global.Rmd")
```

<br>

## Read in data
This code block will use the tabulated file created in Step 3 and a list of samples; it loads the cleaned global and local databases.
```{r}
input_path <- file.path(params$data_path, params$project_code, "merged_runs", sprintf("%s_%s", params$step_3a_analysis_date, params$user))

data_global <- read.csv(file.path(input_path, sprintf("%s_global_alluniq.clean.tag.ann.tab", params$step_3b_analysis_date)), sep="\t", header=TRUE)

data_local <- read.csv(file.path(input_path, sprintf("%s_local_alluniq.clean.tag.ann.tab", params$step_3c_analysis_date)), sep="\t", header=TRUE)

data_samples <- read.csv(params$metadata_file)
```

<br>

### Collect some insights on the shape of data
Are there the same number of sequences in your local and global files?
```{r}
if (length(which(data_local$id != data_global$id)) == 0){
  print("Your files have the same number of sequences.")
} else {
  print("There is a mismatch in file lengths. Please check inputs and rerun Step 3.")
}
```

<br>

### Rename the default column X to `r params$sampleID_column` 
```{r}
colnames(data_samples)[colnames(data_samples) == "X"] = params$sampleID_column
```

<br>

This block of code will read in the 'completeDB.csv' global library output of step 2a, trim this file to sequence, merged_family, merged_genus, and merged_species columns, and join this file to 'data_global' by sequence.
```{r}
# Read in .csv file from step 2a
global_detailed_headers <- read.csv(file.path(params$global_ref_lib_path, params$taxa_division, params$locus, params$region_of_interest, params$latest_db_date, paste0(params$step_2a_analysis_date, "_", params$region_of_interest, "_completeDB.csv")))
head(global_detailed_headers)

# Trim columns 
global_detailed_headers <- data.frame(sequence = global_detailed_headers$sequence, merged_family_name = global_detailed_headers$merged_family_name, merged_genus_name = global_detailed_headers$merged_genus_name, merged_species_name = global_detailed_headers$merged_species_name)
ncol(global_detailed_headers)

# Join global_detailed_headers to data_global
data_global <- left_join(data_global, global_detailed_headers, by = "sequence")
```

This block of code will read in the 'completeDB.csv' local library output of step 2b, trim this file to sequence, merged_family, merged_genus, and merged_species columns, and join this file to 'data_local' by sequence.
```{R}
# Read in .csv file from step 2b
local_detailed_headers <- read.csv(file.path(params$local_ref_lib_path, params$project_code, params$locus, params$region_of_interest, params$step_2b_analysis_date, paste0(params$project_code, params$region_of_interest, "_", params$step_2b_analysis_date, "_completeDB.csv")))
head(local_detailed_headers)

# Trim columns
local_detailed_headers <- data.frame(sequence = local_detailed_headers$sequence, merged_family_name = local_detailed_headers$merged_family_name, merged_genus_name = local_detailed_headers$merged_genus_name, merged_species_name = local_detailed_headers$merged_species_name)
ncol(local_detailed_headers)

# Join local_detailed_headers to data_local
data_local <- left_join(data_local, local_detailed_headers, by = "sequence")
```

<br>

### Filter the data to only keep 100% matches in the local set
```{r}
size <- dim(data_local) # number of rows and columns
print(paste0("Number of rows in local data: ", size[1]))
print(paste0("Number of columns in local data: ", size[2]))

best_id_col_local <- extract_column_names(data_local, starts_with("best_identity"))
best_match_col_local <- extract_column_names(data_local, starts_with("best_match"))

data_local <- data_local %>% 
  select(-contains("obiclean_status"))

# filter for percent match
local_keep <- data_local[which(data_local[[best_id_col_local]]==params$percent_match),]

local_keep_rows <- nrow(local_keep)
print(paste0("Number of local rows kept: ", local_keep_rows))
```

<br>

### Filter the data to only keep 100% matches in the global set
Global only keeps perfect matches if they are not already present in the local set of matches.
```{r}
size <- dim(data_global) # number of rows and columns
print(paste0("Number of rows in global data: ", size[1]))
print(paste0("Number of columns in global data: ", size[2]))

best_id_col <- find_columns(data_global, starts_with("best_identity"))
best_match_col <- find_columns(data_global, starts_with("best_match"))

data_global <- data_global %>% 
  select(-contains("obiclean_status"))
  
# filter for any new matches at the percent match threshold if they are not found in the local set already
global_keep <- data_global[intersect(which(data_global[[best_id_col]] == params$percent_match), which(data_local[[best_id_col_local]]!= params$percent_match)),]

global_keep$scientific_name <- make.unique(as.character(global_keep$scientific_name))

print(paste0("Number of global rows kept: ", nrow(global_keep)))
```

<br>

### Define a function to filter the data to samples with at least `r params$min_read_count`
```{r}
filter_low_count_samples <- function(df) {
  # calc read count sums for sample columns
  sample_sums <- df %>% 
    select(starts_with("sample.")) %>% 
    summarise(across(everything(), \(x) sum(x, na.rm = TRUE)))
  
  # get names of columns to keep (those with sums >= `min_read_count`)
  columns_to_keep <- sample_sums %>%
    pivot_longer(everything(), names_to = "column", values_to = "sum") %>%
    filter(sum >= params$min_read_count) %>%
    pull(column)
  
  # get all non-sample columns to keep
  non_sample_cols <- names(df)[!grepl("^sample\\.", names(df))]
  
  # return the filtered df with only samples passing `min_read_count`
  df %>% select(all_of(non_sample_cols), any_of(columns_to_keep))
}
```

### Combine the local and global datasets and filter by `min_read_count`
Combine local and additional global libraries into a database called "keep" for exploratory analyses.
```{r}
# Add missing columns to global before overwriting column names
global_keepKEEP <- global_keep %>%
    mutate(merged_family_name = "", 
           merged_genus_name = "",
           merged_species_name = "",
           .after = "sequence")

# Overwrite with local names
names(global_keepKEEP) <- names(local_keep)
keep <- rbind(local_keep, global_keepKEEP)
# filter for min read count
keep <- filter_low_count_samples(keep)
```

<br>

### Explore the distribution of sequences
Find out distribution of the sequences you toss both locally and globally (i.e., do some match multiple species perfectly, or anything new in GenBank?).
```{r}
local_global_toss <- data_local[which(data_local$id %in% keep$id==F),] # tells you how many taxa didn't have 100% matches 

print(paste0("The number of sequences not matching at 100%: ", dim(local_global_toss)[1]))
```

<br>

### Look at the seqs with highest read counts for matches >98%
```{r}
local_global_toss <- local_global_toss[order(local_global_toss$count,decreasing=T),]
head(local_global_toss)
# may be a couple of taxa we should look at that have 1-2 matches at ~98% match with high read count 
```

<br>

### Look at a histogram of %match < 1 for the tossed samples
```{r warning=FALSE}
ggplot(local_global_toss, aes_string(x = best_id_col_local)) +
  geom_histogram(binwidth=0.05, fill = "steelblue", color = "black") +
  labs(title = "Histogram of Best ID Column Local",
       x = best_id_col_local,
       y = "Frequency") +
  scale_x_continuous(
  breaks = seq(0, 1, by = 0.1),
  labels = scales::percent_format(scale = 100)
) + 
  theme_minimal()
```

<br>

### Count up 100% matches in each family with the local library
```{r}
local_keep %>% count(family_name, sort = TRUE)
```

<br>

### Count up 100% matches in each genera with the local library
```{r}
local_keep %>% count(genus_name, sort = TRUE)
```

<br>

### Count up 100% matches in each family with the global library
```{r}
global_keep_families <- global_keep %>% count(family_name, sort = TRUE)
global_keep_families
write.csv(global_keep_families, file.path(input_path, "global_keep_familes.csv"))
```

<br>

### Count up 100% matches in each genera with the global library
```{r}
global_keep_genera <- global_keep %>% count(genus_name, sort = TRUE)
global_keep_genera
write.csv(global_keep_genera, file.path(input_path, "global_keep_genera.csv"))
```

<br>

## Inspect the 90-99% matches
```{r}
local_nineties <- filter(data_local, between(data_local[[best_id_col_local]], 0.9, 1)) %>%
  arrange(desc(best_id_col_local))
local_nineties %>% count(family_name, sort = TRUE) # families with lots of reads matching at 90-99.99% 
```

```{r}
global_nineties <- filter(data_global, between(data_global[[best_id_col]], 0.9, 1)) %>%
  arrange(desc(best_id_col))
global_nineties %>% count(family_name, sort = TRUE) # families with lots of reads matching at 90-99.99% 
```

<br>

Filter for records in the global that are a 100% match not already matched at 100% in the local db.
```{r}
global_nineties_keep <- global_nineties[intersect(which(global_nineties[[best_id_col]] == params$percent_match), which(data_local[[best_id_col_local]] != params$percent_match)),]
```

<br>

## Generate summary statistics for publication
#### Median, mean, and max for local and global counts
```{r}
# Mean read depth of taxa identified using the local reference library
print(paste0("The mean depth across all taxa for the local keep subset: ", round(mean(local_keep$count))))

# Mean read depth of taxa identified using the global reference library
print(paste0("The mean depth across all taxa for the global keep subset: ", round(mean(global_keep$count))))

# Median read depth of taxa identified using the local reference library
print(paste0("The median depth across all taxa for the local keep subset: ", round(median(local_keep$count))))

# Median read depth of taxa identified using the global reference library
print(paste0("The median depth across all taxa for the global keep subset: ", round(median(global_keep$count))))

# Maximum read depth of taxa identified using the local reference library
print(paste0("The maximum depth for any one taxa in the local keep subset: ", max(local_keep$count))) 

# Maximum read depth of taxa identified using the global reference library
print(paste0("The maximum depth for any one taxa in the global keep subset: ", max(global_keep$count)))
```

<br>

### Comparison of median read depths across local and global reference libraries
```{r}
## This assumes your local reference has better median read depth than the global. Some projects may still have a small local database relative to global, so you could switch global and local in the division below, and make sure to report that the global is x-fold greater than local.
print(paste0("Median read depths for the local reference library were >",round(median(local_keep$count)/median(global_keep$count)),"-fold greater than the global reference library"))

# Calculate total number of reads in final dataset that were taxonomically assigned using each reference library
print(paste0("Total number of reads in the global keep subset: ", sum(global_keep$count)))
print(paste0("Total number of reads in the local keep subset: ", sum(local_keep$count)))
print(paste0("Total number of reads in the final dataset: ", sum(global_keep$count) + sum(local_keep$count)))
```

<br>

#### Sample read depths
This will make tables with the depths for each sample in each data subset.
```{r}
depth <- as.data.frame(t(colSums(data_local[,grep("sample\\.", colnames(data_local))])))
depth_local_only <- as.data.frame(t(colSums(local_keep[,grep("sample\\.", colnames(local_keep))])))
depth_global_only <- as.data.frame(t(colSums(global_keep[,grep("sample\\.", colnames(global_keep))])))
depth_local_and_global <- as.data.frame(t(colSums(keep[,grep("sample\\.", colnames(keep))])))

# Combine into one result df; view the new 'sample_depth' dataframe for comparisons
sample_depths <- bind_rows(
  depth = depth,
  depth_local_only = depth_local_only,
  depth_global_only = depth_global_only,
  depth_local_and_global = depth_local_and_global,
  .id = "source"
)
```

<br>

#### Proportions discarded with each subset
```{r}
sample_depths <- sample_depths %>%
  pivot_longer(cols = -source, names_to = "sample", values_to = "value") %>%
  pivot_wider(names_from = source, values_from = value) %>%
  group_by(sample) %>%
  mutate(prop_kept_local = depth_local_only / depth,
         prop_kept_global = depth_global_only / depth,
         prop_kept_both = depth_local_and_global / depth,
         reads_discarded = 1 - prop_kept_both,
         !!sym(params$sampleID_column) := substring(sample, 8, 13)) %>%
  ungroup() %>%
  left_join(data_samples %>%
              select(!!sym(params$sampleID_column), !!sym(params$major_grouping)), by = setNames(params$sampleID_column, params$sampleID_column))

write.csv(sample_depths, "./sample_depths.csv")
sample_depths 
```
<br>

#### Plot proportional changes at sample level if you add or don't add the global perfect matches to the local perfect matches
```{r}

ggplot(sample_depths, aes(x = prop_kept_local, y = prop_kept_both)) +
  geom_point(size=3) +
  geom_text_repel(aes(label = !!sym(params$sampleID_column)),
                  size=3)+
  labs(x = "Proportion Kept Locally", 
       y = "Proportion Kept Both Locally and Globally") +
  scale_y_continuous(
  labels = percent, 
  limits = c(0, 1), 
) +
  scale_x_continuous(
  labels = percent, 
  limits = c(0, 1), 
) +
  theme_minimal()
```

<br>

#### Plot proportions from each subset by major gouping
```{r warning=FALSE}
sample_depths %>% 
  gather(type, count, prop_kept_local:prop_kept_global:reads_discarded) %>% 
  filter(type != "prop_kept_both") %>%
  ggplot(., aes(x=sample, y=count, fill=forcats::fct_rev(type))) +
  scale_fill_manual(values = c("grey50", "#FFC20A", "#0C7BDC")) +
  geom_bar(stat="identity") +
  theme_classic()+
  theme(legend.title=element_blank(),
        axis.text.x=element_blank()) +
  scale_y_continuous(
  labels = percent, 
  limits = c(0, 1), 
  expand = expansion(0, 0)
) +
  facet_grid(reformulate(params$major_grouping), 
           scales = "free", space = "free") + 
  theme(strip.text = element_text(
    size = 5))
```

<br>

#### Print some important metrics overall
```{r}
#should be 90% to 80%
print(paste0("Overall proportion of reads retained with local and global combined: ", sprintf(sum(sample_depths$depth_local_and_global)/sum(sample_depths$depth),fmt='%#.3f')))

#should be 67% to 61%
print(paste0("Proportion of reads retained with just local: ", sprintf(sum(sample_depths$depth_local_only)/sum(sample_depths$depth),fmt='%#.3f')))

#should be 23% to 27%
print(paste0("Additional proportion of reads retained with just global: ", sprintf(sum(sample_depths$depth_global_only)/sum(sample_depths$depth),fmt='%#.3f')))
```

<br>

#### 100% matches in local that were new to global
```{r}
global_match <- data_global[which(data_global[[best_id_col]] == 1),]

global_missing <- subset(local_keep, !(id %in% global_match$id)) %>%
  arrange(desc(count))

global_missing_trim <- select(global_missing, starts_with("sample."))

global_missing_RRA <- as.matrix(global_missing_trim) %o% as.matrix(1/sample_depths$depth_local_and_global)

global_missing_RRA_max <- round(apply(global_missing_RRA, 1, max, na.rm=TRUE), digits=6)
global_missing_RRA_mean <- round(apply(global_missing_RRA, 1, mean, na.rm=TRUE), digits=6)

global_missing_RRA_info <- cbind(global_missing %>% select(sequence, taxid, species_name, count, genus_name, family_name, starts_with(c("species_list", "best_", "match_"))), global_missing_RRA_max, global_missing_RRA_mean)
global_missing_RRA_info
```

<br>

## Sample-wise Relative Read Abundance
Sample-wise distribution of reads retained/discarded based on local and/or global criteria.
```{r}
# Calculate range of RRA change when global matches are added in
additional_RRA_with_global <- round(sample_depths$prop_kept_both - sample_depths$prop_kept_local, digits=3)
RRA_range_global <-  range(additional_RRA_with_global)
print(glue("Range of RRA with global: ", RRA_range_global[1], "-", RRA_range_global[2]))
print(glue("Median RRA with global across samples: ", median(additional_RRA_with_global)))
print(glue("Mean RRA with global across samples: ", mean(additional_RRA_with_global)))

# If >75% of samples gain less than 1% of reads back by including the global, we're in good shape; however, a subset of samples (<5%) could gain back a rather high percentage (>61%), suggesting that we should facilitate
print(glue("Quantiles of RRA with global: "))
quantile(additional_RRA_with_global,c(0.75, 0.95,1))
```

<br>

## Disproportionality
Figure out whether any of the global matches contribute disproportionately. Report global-matching-taxa that contribute more than 5% of a sample's reads; within those samples, what are the mean, median, and max values?
```{r}
# This block makes an empty matrix with as many rows as we need for each sequence identified in the global_keep subset
global_keep_plants <- matrix(nrow=nrow(global_keep), ncol=4)
rownames(global_keep_plants) <- as.character(global_keep$scientific_name)
colnames(global_keep_plants) <- c("length", "mean", "median", "maximum")
```

<br>

This block will loop through each taxa and calculate metrics for each taxa that has at least 1 sample with RRA > RRA_threshold.  
```{r}
for(i in 1:nrow(global_keep_plants)){
  sample_cols = global_keep[i,grep("sample\\.", colnames(global_keep))]
  global_keep_plants[i,1] <- length(which((sample_cols/depth) > params$RRA_threshold))
    
  if(global_keep_plants[i, "length"] >= 1){
    
      global_keep_plants[i, "mean"] <- mean(as.numeric(sample_cols[which((sample_cols/depth) > params$RRA_threshold)])  /  as.numeric(depth[which((sample_cols/depth) > params$RRA_threshold)]))
    
      global_keep_plants[i, "median"] <- median(as.numeric(global_keep[i, grep("sample\\.", colnames(global_keep))][which((sample_cols/depth) > params$RRA_threshold)])  /  as.numeric(depth[which((sample_cols/depth) > params$RRA_threshold)]))
    
      global_keep_plants[i, "maximum"] <- max(as.numeric(global_keep[i, grep("sample\\.", colnames(global_keep))][which((sample_cols/depth) > params$RRA_threshold)])  /  as.numeric(depth[which((sample_cols/depth) > params$RRA_threshold)]))
  }
}

global_keep_plants <- as.data.frame(global_keep_plants) %>% drop_na()
```

<br>

Print out some overall metrics about disproportionality in the dataset.
```{r}
print(glue("Number of disproportionate taxa: ", nrow(global_keep_plants)))

range_of_dis_lengths <- range(global_keep_plants$length)
print(glue("Range of sequence counts: ", range_of_dis_lengths[1], "-", range_of_dis_lengths[2]))

range_of_dis_means <- round(range(global_keep_plants$mean), 3)
print(glue("Range of means: ", range_of_dis_means[1], "-", range_of_dis_means[2]))

range_of_dis_maxs <- round(range(global_keep_plants$maximum), 3)
print(glue("Range of maximums: ", range_of_dis_maxs[1], "-", range_of_dis_maxs[2]))
```

<br>

## Summarize local and non-trace global
Make a new dataframe containing the local matches and the non-trace global matches and calculate summary stats. Non-trace global matches = higher abundance global matches (e.g. >5%).
```{r}
# Add missing columns to global before overwriting column names
global_nontrace <- global_keep[which(global_keep$scientific_name%in%rownames(global_keep_plants)),] %>%
    mutate(merged_family_name = "", 
           merged_genus_name = "",
           merged_species_name = "",
           .after = "sequence")
colnames(global_nontrace) <- names(local_keep)
global_nontrace 
local_nontrace_global <- rbind(local_keep, global_nontrace)
lng_dims <- dim(local_nontrace_global)
print(glue("There are ", lng_dims[1], " sequence records in the local + global_nontrace subset and ", lng_dims[2], " columns."))
```

<br>

Calculate sample-wise retention of RRA.
```{r}
local_nontrace_global_depth <- colSums(local_nontrace_global[,grep("sample\\.", colnames(local_nontrace_global))])

proportion_depth_kept_lng <- local_nontrace_global_depth/as.matrix(depth)
as.data.frame(proportion_depth_kept_lng)
```

<br>

Print metrics about sample-wise retention of RRA.
```{r}
print(glue("Quantiles of RRA with local + global nontrace: "))
quantile(proportion_depth_kept_lng)

# proportion of overall reads kept
print(glue("\n       
Total local + global notrace depth: ", sum(local_nontrace_global_depth)))
print(glue("Total local + global notrace depth retained: ", round(sum(local_nontrace_global_depth)/sum(as.matrix(depth)), 3)))
```

<br>

## Summary statistics for sample groupings
```{r}
# Clean up sample names by removing "sample. and _Sx"
depth_data <- tibble(
  SampleID = str_remove(names(local_nontrace_global_depth), "^sample\\."),
  local_nontrace_global_depth = local_nontrace_global_depth
) %>%
  mutate(SampleID = str_remove(SampleID, "_S\\d+$"))

# Join with metadata
combined_data <- data_samples %>%
  left_join(depth_data, by = "SampleID")

# Calculate stats by group
local_nontrace_global_stats <- combined_data %>%
  group_by(.data[[params$minor_grouping]], .data[[params$major_grouping]]) %>%
  summarize(
    mean = mean(local_nontrace_global_depth, na.rm = TRUE),
    sd = sd(local_nontrace_global_depth, na.rm = TRUE),
    count = n(),
    .groups = "drop"
  )
local_nontrace_global_stats
```

<br>

## Save outputs and generate Phyloseq object
```{r}
# Make the OTU table.
OTU_table <- keep %>%
  select(starts_with("sample.")) %>%
  rename_with(~ str_remove_all(., "sample.|_S\\d+"), starts_with("sample.")) %>%
  as_tibble(rownames = "row_id") %>%
  mutate(row_id = paste0("P", row_id)) %>%
  column_to_rownames("row_id")

# Make the Taxa table.
Taxa_table <- keep %>%
  select(-starts_with("sample.")) %>%
  relocate(id, definition, all_of(best_id_col_local), all_of(best_match_col_local), count, .after = last_col()) %>%
  as_tibble(rownames = "row_id") %>%
  mutate(row_id = paste0("P", row_id)) %>%
  column_to_rownames("row_id")

# Set the index of the samples data to sample names; Phyloseq needs this to match up the different shapes of OTU, TAX, and Samples tables
rownames(data_samples) <-  data_samples[[params$sampleID_column]]
```

## Calulate per-sample number of taxa
```{r}
per_sample_avg <- OTU_table %>%
  pivot_longer(cols = -1, names_to = "sample", values_to = "value") %>%
  group_by(sample) %>%
  summarize(
    num_taxa = sum(value > 0),
  ) %>%
  arrange(sample)
print(per_sample_avg)
```
## Calculate mean, median, min, and max number of taxa
```{r}
# mean number of taxa across samples
print(paste0("The mean number of taxa across samples: ", round(mean(per_sample_avg$num_taxa))))

# median number of taxa across samples
print(paste0("The median number of taxa across samples: ", round(mean(per_sample_avg$num_taxa))))

# min number of taxa across samples
min_taxa <- min(per_sample_avg$num_taxa)
min_sample <- per_sample_avg$sample[which.min(per_sample_avg$num_taxa)]
print(paste0("The minimum number of taxa in any one sample: ", min_taxa, " in ", min_sample))

# max number of taxa across samples
max_taxa <- max(per_sample_avg$num_taxa)
max_sample <- per_sample_avg$sample[which.max(per_sample_avg$num_taxa)]
print(paste0("The maximum number of taxa in any one sample: ", max_taxa, " in ", max_sample))
```

<br>

## Check sample order in metadata and OTU tables
```{r}
# View the names of SampleIDs in the metadata and OTU table data frames
names(OTU_table)
data_samples$SampleID
```

```{r}
# Check for differences between the two lists 
setdiff(names(OTU_table), data_samples$SampleID) # This tells us which samples are present in first set that aren't present in second set - if you get output here, we need to clean up names 
setdiff(data_samples$SampleID, names(OTU_table)) # This tells us which samples are present in second set that aren't present in first set - if you get output here, we need to clean up names 

# These two lines will remove any 'lab' SampleIDs that may have the "_S##" format from the sequencing machine
names(OTU_table) <- str_replace(names(OTU_table), "_.*$", "") 
data_samples$SampleID <- str_replace(data_samples$SampleID, "_.*$", "") 

# Check to see that name issues have been fixed (if you get output here besides 'character(0)', you have not successfully fixed names)
setdiff(names(OTU_table), data_samples$SampleID) 

# Set the order of SampleIDs in metadata file to match the order of SampleIDs in OTU table. Open both files in 'Environment' pane to view and check both data frames.
data_samples <- data_samples[order(match(data_samples$SampleID, names(OTU_table))), ]
```

<br>

Build the Phyloseq diet object, rarefy, and export the rarefied tables for easy loading in future.
```{r}
OTU = phyloseq::otu_table(as.matrix(OTU_table), taxa_are_rows = TRUE)
TAX = phyloseq::tax_table(as.matrix(Taxa_table))
samples = phyloseq::sample_data(data_samples)

filename <- glue({params$project_code}, "_diet_local_global_", {params$todaydate}, ".rds")
phyloseq_object <- phyloseq::phyloseq(OTU, TAX, samples)
phyloseq_object

saveRDS(phyloseq_object, filename)

# WE HAVE A NEW PHYLOSEQ OBJECT!!!
```
<br>

## Print params
```{r}
params
```
### TODO: Copy over the html notebook
