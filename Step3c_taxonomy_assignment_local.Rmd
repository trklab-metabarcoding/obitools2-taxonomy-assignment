---
title: "Step 3c - assign taxonomy w/ local db"
subtitle: "Assign taxonomy to cleaned sequences using local database file."
authors: "B.Littleford-Colquhoun & T.Divoll"
date: "2024-04-25"
output: 
  html_notebook:
  keep_md: TRUE
  df_print: paged
  toc: true
params:
    #don't change these
    user: !r Sys.getenv("LOGNAME")
    data_path: "/oscar/data/tkartzin/projects"
    ref_lib_path: "/oscar/data/tkartzin/local_ref_lib"
  
    #update these for your analysis
    step_3a_analysis_date: "20240529" #YYYYMMDD
    step_3c_analysis_date: "20240530" #YYYYMMDD
    reference_lib_project_code: "YNP"
    data_project_code: "test"
    taxa_division: "PLN"
    locus: "trnL"
    region_of_interest: "P6"
    latest_db_date: "20240430" #look in `ref_lib_path/project_code/locus/region_of_interest` for the most recent local reference library you want to use #
---

This notebook can be used to run the OBITools 1.2.12 pipeline and serve as a tutorial. Run through each code chunk and inspect the outputs. We assume that you are running this notebook on the same day that you ran Step 3a.

```{r include=FALSE}
## Only needed for knitr to render this notebook
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, here, tidyr, dplyr, readxl, stringr, spgs, filesstrings)
here::i_am("./Step3c_taxonomy_assignment_local.Rmd")
```

#### Set up standard path variables
The following variables are created with the params you input in the YAML block above.
```{r}
output_path <- file.path(params$data_path, params$data_project_code, "merged_runs", sprintf("%s_%s", params$step_3a_analysis_date, params$user))

cleaned_obitools_fasta <- file.path(output_path, sprintf("%s_alluniq.clean.fasta", params$step_3a_analysis_date))

local_ref_db <- file.path(params$ref_lib_path, params$reference_lib_project_code, params$locus, params$region_of_interest, params$latest_db_date, sprintf("formatdb_%s_%s", params$reference_lib_project_code, params$locus), sprintf("%s_%s_%s", params$reference_lib_project_code, params$locus, params$latest_db_date)) 
#-d in ecotag; ecoPCR taxonomy Database name.

ref_seq <- file.path(params$ref_lib_path, params$reference_lib_project_code, params$locus, params$region_of_interest, params$latest_db_date, sprintf("%s%s_completeDB_%s.fasta", params$reference_lib_project_code, params$region_of_interest, params$latest_db_date)) 
#-R in ecotag; fasta file containing the reference sequences.
```

```{r}
## make environment variables that can be passed to bash code chunks
# set global chunk parameters here
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(cleaned_obitools_fasta=cleaned_obitools_fasta)
Sys.setenv(local_ref_db=local_ref_db)
Sys.setenv(ref_seq=ref_seq)
Sys.setenv(user=params$user)
Sys.setenv(DATE=params$step_3c_analysis_date)
Sys.setenv(output_path=output_path)
```

#### Assign taxonomy to each sequence using the global reference library
```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
mkdir -p $output_path
cd $output_path

ecotag -d ${local_ref_db} -R ${ref_seq} ${cleaned_obitools_fasta} > ${DATE}_local_alluniq.clean.tag.fasta

#Count number of sequences in your file
obicount ${DATE}_local_alluniq.clean.tag.fasta --without-progress-bar
```

Un-useful attributes can be removed from sequences
```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obiannotate --delete-tag=scientific_name_by_db --delete-tag=obiclean_samplecount \
  --delete-tag=obiclean_count --delete-tag=obiclean_singletoncount \
  --delete-tag=obiclean_cluster --delete-tag=obiclean_internalcount \
  --delete-tag=obiclean_head --delete-tag=taxid_by_db --delete-tag=obiclean_headcount \
  --delete-tag=id_status --delete-tag=rank_by_db --delete-tag=order_name \
  --delete-tag=order ${output_path}/${DATE}_local_alluniq.clean.tag.fasta > \
  ${output_path}/${DATE}_local_alluniq.clean.tag.ann.fasta

#count number of sequences in file (should match above)
obicount -a ${DATE}_local_alluniq.clean.tag.ann.fasta --without-progress-bar
```

### Create output for use in phyloseq
This is your final OBITools output (resembles a large ASV/OTU table) that can be used to create a phyloseq object in the next step
```{bash}
source ~/.bash_profile
conda activate /users/${user}/.conda/envs/obitools-env
cd $output_path

obitab -o ${DATE}_local_alluniq.clean.tag.ann.fasta > ${DATE}_local_alluniq.clean.tag.ann.tab
```

#### Copy over your notebook to the shared lab directory on Oscar.

**Note: Save this notebook first so any results are recorded before copying. Then run this final chunk.**
```{r}
file.copy("./Step3c_taxonomy_assignment_local.nb.html", file.path(glue("{output_path}/Step3c_taxonomy_assignment_local.nb.html")), overwrite = TRUE)
```


## Merge the two tabulated files together. 1) ref_seq (the output from step 2c) and 2) the tabulated file that you created above with the header "Create output for use in phyloseq." 
```{r}
ref_seq_tab_2c <- "/oscar/data/tkartzin/projects/YNP/merged_runs/20241119_hhoff/20241120_ref_seq.tab"
ref_seq_tab_2c <- read.table(ref_seq_tab_2c,  sep="\t", header = TRUE)
head(ref_seq_tab_2c)

output_tab_3c <- "/oscar/data/tkartzin/projects/YNP/merged_runs/20241119_hhoff/20241120_local_alluniq.clean.tag.ann.tab"
output_tab_3c <- read.table(output_tab_3c,  sep="\t", header = TRUE)
head(output_tab_3c)

merged_3c_for_phyloseq <- left_join(output_tab_3c, ref_seq_tab_2c, by = "sequence")

write.table(merged_3c_for_phyloseq, "20241125_local_alluniq.clean.tag.ann.tab.headers")
```
