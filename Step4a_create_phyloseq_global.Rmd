---
title: "Step 4a - Phyloseq global"
subtitle: "Create phyloseq object using global output only."
authors: "TR Kartzinel, BL Littleford-Colquhoun, HK Hoff, TJ Divoll"
date: "2024-04-25"
output: 
  html_notebook:
  keep_md: TRUE
  df_print: paged
  toc: true
params:
    #don't change these
    todaydate: !r (format(Sys.Date(), "%Y%m%d"))
    user: !r Sys.getenv("LOGNAME")
    data_path: "/oscar/data/tkartzin/projects"
    ref_lib_path: "/oscar/data/tkartzin/global_ref_lib"
 
    #update these for your analysis
    project_code: "test"
    taxa_division: "PLN"
    locus: "trnL"
    region_of_interest: "P6"
    latest_db_date: "20241125"
    step_2a_analysis_date: "20241227" #look in the `latest_db_date` folder and get the date associated with the `_completeDB.csv` file
    step_3a_analysis_date: "20241123" #look in tkartzin > projects > project_code > merged_runs > date on folder YYYYMMDD_username
    step_3b_analysis_date: "20240502" #if running on the same day as Step 3, this will be `todaydate`, otherwise enter the date when you ran Step 3b
    metadata_file: "test_phyloseq_metadata.csv"
    percent_match: 1 # value between 0-1; percent match between reference sequence and your sequence
---

## Create a Phyloseq Object
This notebook will use your data with taxonomy assigned from the latest global reference database to create a phyloseq object for downstream analyses. 
```{r setup, include=FALSE}
## Only needed for knitr to render this notebook
knitr::opts_chunk$set(echo = TRUE)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, here, tidyr, dplyr, phyloseq, vegan, glue, scales, datawizard) #this takes a few minutes
here::i_am("./Step4a_create_phyloseq_global.Rmd")
```

## Read in data
This code block will use the tabulated file created in Step 3 and a list of samples
```{r}
input_path <- file.path(params$data_path, params$project_code, "merged_runs", sprintf("%s_%s", params$step_3a_analysis_date, params$user))

data_global <- read.csv(file.path(input_path, sprintf("%s_global_alluniq.clean.tag.ann.tab", params$step_3b_analysis_date)), sep="\t", header=TRUE)

data_samples <- read.csv(params$metadata_file)
data_samples <- data_samples[order(data_samples$SampleID), ]
```

This block of code will read in the 'completeDB.csv' output of step 2a, trim this file to sequence, merged_family, merged_genus, and merged_species columns, and join this file to 'data_global' by sequence.
```{r}
#Read in .csv file from step 2a
global_detailed_headers <- read.csv(file.path(params$ref_lib_path, params$taxa_division, params$locus, params$region_of_interest, params$latest_db_date, paste0(params$step_2a_analysis_date, "_", params$region_of_interest, "_completeDB.csv")))
head(global_detailed_headers)

#Trim columns 
global_detailed_headers <- data.frame(sequence = global_detailed_headers$sequence, merged_family_name = global_detailed_headers$merged_family_name, merged_genus_name = global_detailed_headers$merged_genus_name, merged_species_name = global_detailed_headers$merged_species_name)
ncol(global_detailed_headers)

#Join global_detailed_headers to data_global
data_global <- left_join(data_global, global_detailed_headers, by = "sequence")
```

### Filter the data to only keep 100% matches
```{r}
size <- dim(data_global) #number of rows and columns
print(paste0("Number of rows in data: ", size[1]))
print(paste0("Number of columns in data: ", size[2]))

best_id_col <- find_columns(data_global, starts_with("best_identity"))
best_match_col <- find_columns(data_global, starts_with("best_match"))

data_global <- data_global %>% 
  select(-contains("obiclean_status"))
  
global_keep <- data_global[which(data_global[,best_id_col]==1),]

global_keep$scientific_name <- make.unique(as.character(global_keep$scientific_name))

keep_rows <- nrow(global_keep)
print(paste0("Number of rows kept: ", keep_rows))
```

## Make a table to represent the counts of each OTU in each sample
```{r}
OTU_table <- global_keep[,grep("sample\\.", colnames(global_keep))]
names(OTU_table) <- gsub(pattern = "sample.", replacement = "", x = names(OTU_table))
rownames(OTU_table) <- sub("^", "P", rownames(OTU_table))
```

## Make a taxonomy table
```{r}
tax_table <- global_keep[,-grep("sample\\.", colnames(global_keep))]
tax_table <- relocate(tax_table, id, definition, {{ best_id_col }}, {{ best_match_col }}, count, .after = last_col())
tax_table2 <- as.matrix(tax_table)
rownames(tax_table2) <- sub("^", "P", rownames(tax_table2))
```

## Do some cleaning before making the final Phyloseq object
```{r}
rownames(data_samples) <- data_samples$SampleID
samples <- sample_data(data_samples)
OTU <- otu_table(OTU_table, taxa_are_rows = TRUE)
TAX <- tax_table(tax_table2)

if (identical(sample_names(OTU), sample_names(samples))){
  print("Your samples are in the right order. Proceed with making your final RDS file!")
} else {
  print("Something is out of order. Make sure the samples in your metadata file match the order in your OTU. Please fix and rerun this notebook.")
}
```

## Create the final Phyloseq object
```{r}
rds_file<- phyloseq(OTU, TAX, samples) 
saveRDS(rds_file, file.path(input_path, paste0(params$project_code, params$region_of_interest, "_diet_global_", params$todaydate, ".rds")))
```

## Copy over OTU, TAX, samples to "merged_runs"
```{r}
output_path <- file.path(params$data_path, params$project_code, "merged_runs", sprintf("%s_%s", params$step_3b_analysis_date, params$user))

write.csv(OTU_table, "OTU_table.csv")
file.copy("./OTU_table.csv", file.path(glue("{output_path}/OTU_table.csv")))
write.csv(tax_table2, "TAX_table.csv")
file.copy("./TAX_table.csv", file.path(glue("{output_path}/TAX_table.csv")))
write.csv(data_samples, "./SAMPLES_table.csv")
file.copy("./SAMPLES_table.csv", file.path(glue("{output_path}/SAMPLES_table.csv")))

file.copy("./Step4a_create_phyloseq_global.nb.html", file.path(glue("{output_path}/Step4a_create_phyloseq_global.nb.html")), overwrite = TRUE)
```

